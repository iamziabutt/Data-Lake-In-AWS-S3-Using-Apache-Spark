{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384e2a8d",
   "metadata": {},
   "source": [
    "## Full ETL Project Code\n",
    "\n",
    "---\n",
    "\n",
    "**what does this code do?**\n",
    "\n",
    "- ✅ **Extract**: Ingests data from aws s3 bucket directly using spark.  \n",
    "- ✅ **Extract**: Uses boto3 package to verify connection and extract basic s3 bucket information\n",
    "- ✅ **Tranform**: Performs data transformation to enrich the data\n",
    "- ✅ **Load**: Loads transformed data back to aws 3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding and importing spark on my machine\n",
    "\n",
    "\"\"\"\n",
    "1. Verify that the IAM role or user has the necessary permisisons to read from the S3 bucket\n",
    "2. The policy should include `s3:GetObject` and `s3:ListBucket` (for the bucket) at minimum.\n",
    "3. Check if the bucket policy or any S3 bucket policy denies the access.\n",
    "4. Version of `hadoop-aws` must match the Hadoop version that Spark is built with. In our case, spark is built with hadoop 3.3.4\n",
    "\n",
    "\"\"\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887f048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Hadoop home\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "sys.path.append(os.environ['HADOOP_HOME'] + \"\\\\bin\")\n",
    "\n",
    "# Set Hive temp dirs\n",
    "os.environ['TEMP'] = 'C:\\\\tmp'\n",
    "os.environ['TMP'] = 'C:\\\\tmp'\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col, year, month, sum, avg, count # type: ignore\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "import configparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98f2a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting configurations\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "aws_access_key = config['AWS']['access_key']\n",
    "aws_secret_key = config['AWS']['Secret_key']\n",
    "\n",
    "s3_input_bucket = config['S3']['input_bucket'] # top level container name\n",
    "s3_input_prefix = config['S3']['input_prefix'] # the 'folder path' within the bucket\n",
    "\n",
    "single_file = config['PATH']['sinlge_file_path']\n",
    "songs_files = config['PATH']['all_songs_files']\n",
    "s3_logs_files = config['PATH']['all_log_files']\n",
    "\n",
    "s3_output_songs_table = config['songs_table']['songs_path']\n",
    "s3_output_logs_table = config['log_table']['logs_path']\n",
    "s3_output_combined_table = config['combined_table']['combined_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa1fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get S3 file list using Boto3\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "        aws_access_key_id= aws_access_key,\n",
    "        aws_secret_access_key= aws_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6f90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(\n",
    "        Bucket=s3_input_bucket,\n",
    "        Prefix=s3_input_prefix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 files to process\n"
     ]
    }
   ],
   "source": [
    "# finding number of files in the bucket \n",
    "\n",
    "files = [f\"s3a://{s3_input_bucket}/{obj['Key']}\" \n",
    "             for obj in response.get('Contents', []) \n",
    "             if not obj['Key'].endswith('/')]\n",
    "\n",
    "print(f\"Found {len(files)} files to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ECRXTEYZB483QZJG',\n",
       "  'HostId': 'XM1QMA8G6n3xnnyHaWnEQ0UYzxZawM2/9Hw69rPYha2RyaLGcoqDiFDFNf9Es0BrbaYFf9rdVZA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'XM1QMA8G6n3xnnyHaWnEQ0UYzxZawM2/9Hw69rPYha2RyaLGcoqDiFDFNf9Es0BrbaYFf9rdVZA=',\n",
       "   'x-amz-request-id': 'ECRXTEYZB483QZJG',\n",
       "   'date': 'Mon, 23 Jun 2025 09:14:49 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'LocationConstraint': 'eu-west-2'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting bucket location\n",
    "\n",
    "s3.get_bucket_location(Bucket=s3_input_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d9e6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9DAW4FJV534Y4QF6',\n",
       "  'HostId': '0HEXANQj5AKoODtXAuEld9IZaxYvhtWaY+GPiWwDZ1TNz6ymXIT39f/77JfRfBe8Q5RbiF0jW2mKD/NebqZWXwWv29x3GFEH',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '0HEXANQj5AKoODtXAuEld9IZaxYvhtWaY+GPiWwDZ1TNz6ymXIT39f/77JfRfBe8Q5RbiF0jW2mKD/NebqZWXwWv29x3GFEH',\n",
       "   'x-amz-request-id': '9DAW4FJV534Y4QF6',\n",
       "   'date': 'Mon, 23 Jun 2025 09:14:52 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Policy': '{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"Statement1\",\"Effect\":\"Allow\",\"Principal\":\"*\",\"Action\":\"s3:*\",\"Resource\":\"arn:aws:s3:::spark-datalake-bucket\"}]}'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting bucket policy\n",
    "\n",
    "s3.get_bucket_policy(Bucket=s3_input_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ad23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing Spark Session with enhanced S3 configurations\n",
    "\n",
    "spark =SparkSession.builder \\\n",
    "        .appName(\"S3-403-Fix\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{\"eu-west-2\"}.amazonaws.com\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.region\", \"eu-west-2\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.acl.default\", \"BucketOwnerFullControl\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"C:/spark-warehouse\") \\\n",
    "        .config(\"hive.exec.scratchdir\", \"C:/tmp/hive\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"20\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"100000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.change.detection.version.required\", \"false\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "|         artist_id|artist_latitude|artist_location|artist_longitude|artist_name| duration|num_songs|           song_id|           title|year|\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "|ARD7TVE1187B99BFB1|           NULL|California - LA|            NULL|     Casual|218.93179|        1|SOMZWCG12A8C13C480|I Didn't Mean To|   0|\n",
      "+------------------+---------------+---------------+----------------+-----------+---------+---------+------------------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading a sinlge file\n",
    "\n",
    "single_song = spark.read.json(single_file) # path is saved in config.ini file\n",
    "\n",
    "single_song.printSchema()\n",
    "single_song.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25536f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading all files in s3 song_data directory\n",
    "\n",
    "all_songs = spark.read.option(\"recursiveFileLookup\", \"true\").json(songs_files)\n",
    "all_songs.printSchema()\n",
    "all_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e4f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary sql view for data processing\n",
    "\n",
    "all_songs.createOrReplaceTempView(\"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7977a1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|          song_title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|ARDNS031187B9924F0|2005|186.48771|\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|ARDR4AC1187FB371A1|   0|511.16363|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (...|AREBBGV1187FB523D2|   0|173.66159|\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...|ARPBNLO1187FB3D52F|2000| 43.36281|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "|SONWXQJ12A8C134D94|The Ballad Of Sle...|ARNF6401187FB57032|1994|  305.162|\n",
      "|SODUJBS12A8C132150|Wessex Loses a Bride|ARI2JSK1187FB496EF|   0|111.62077|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (...|ARPFHN61187FB575F6|   0|279.97995|\n",
      "|SOGXHEG12AB018653E|It Makes No Diffe...|AR0RCMP1187FB3F427|1992|133.32853|\n",
      "|SOBZBAZ12A6D4F8742|      Spanish Grease|AROUOZZ1187B9ABE51|1997|168.25424|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|ARLTWXK1187FB5A3F8|   0|326.00771|\n",
      "|SOZHPGD12A8C1394FE|     Baby Come To Me|AR9AWNF1187B9AB0B4|   0|236.93016|\n",
      "|SOGNCJP12A58A80271|Do You Finally Ne...|ARB29H41187B98F0EF|1972|342.56934|\n",
      "|SOWTBJW12AC468AC6E|Broken-Down Merry...|ARQGYP71187FB44566|   0|151.84934|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F|   0| 267.7024|\n",
      "|SOTTDKS12AB018D69B|It Wont Be Christmas|ARMBR4Y1187B9990EB|   0|241.47546|\n",
      "|SOBLGCN12AB0183212|James (Hold The L...|AR47JEX1187B995D81|1985|124.86485|\n",
      "|SOHKNRJ12A6701D1F8|        Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|\n",
      "|SOMVWWT12A58A7AE05|Knocked Out Of Th...|ARQ9BO41187FB5CF1F|   0|183.17016|\n",
      "|SOBKWDJ12A8C13B2F3|Wild Rose (Back 2...|AR36F9J1187FB406F1|   0|230.71302|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Discarding any duplicate songs using distinct function\n",
    "\n",
    "songs_table_query = (spark.sql (\n",
    "    '''\n",
    "    select distinct\n",
    "    song_id,\n",
    "    title as song_title,\n",
    "    artist_id,\n",
    "    year,\n",
    "    duration\n",
    "    from songs\n",
    "\n",
    "    '''\n",
    ")\n",
    ") \n",
    "\n",
    "songs_table_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca257765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|ARPBNLO1187FB3D52F|            Tiny Tim|        New York, NY|       40.71455|       -74.00712|\n",
      "|ARMAC4T1187FB3FA4C|The Dillinger Esc...|   Morris Plains, NJ|       40.82624|       -74.47995|\n",
      "|ARNF6401187FB57032|   Sophie B. Hawkins|New York, NY [Man...|       40.79086|       -73.96644|\n",
      "|ARDNS031187B9924F0|          Tim Wilson|             Georgia|       32.67828|       -83.22295|\n",
      "|AR0RCMP1187FB3F427|    Billie Jo Spears|        Beaumont, TX|       30.08615|       -94.10158|\n",
      "|AROUOZZ1187B9ABE51|         Willie Bobo|New York, NY [Spa...|       40.79195|       -73.94512|\n",
      "|ARI2JSK1187FB496EF|Nick Ingman;Gavyn...|     London, England|       51.50632|        -0.12714|\n",
      "|ARPFHN61187FB575F6|         Lupe Fiasco|         Chicago, IL|       41.88415|       -87.63241|\n",
      "|ARLTWXK1187FB5A3F8|         King Curtis|      Fort Worth, TX|       32.74863|       -97.32925|\n",
      "|ARMBR4Y1187B9990EB|        David Martin|     California - SF|       37.77916|      -122.42005|\n",
      "|AR47JEX1187B995D81|        SUE THOMPSON|          Nevada, MO|       37.83721|       -94.35868|\n",
      "|ARQGYP71187FB44566|        Jimmy Wakely|         Mineola, AR|       34.31109|       -94.02978|\n",
      "|ARB29H41187B98F0EF|       Terry Callier|             Chicago|       41.88415|       -87.63241|\n",
      "|ARQ9BO41187FB5CF1F|          John Davis|        Pennsylvania|       40.99471|       -77.60454|\n",
      "|AR3JMC51187B9AE49D|     Backstreet Boys|         Orlando, FL|       28.53823|       -81.37739|\n",
      "|ARD842G1187B997376|          Blue Rodeo|Toronto, Ontario,...|       43.64856|       -79.38533|\n",
      "|AR36F9J1187FB406F1|      Bombay Rockers|             Denmark|       56.27609|         9.51695|\n",
      "|ARBGXIG122988F409D|          Steel Rain|     California - SF|       37.77916|      -122.42005|\n",
      "|ARGSJW91187B9B1D6B|        JennyAnyKind|      North Carolina|       35.21962|       -80.01955|\n",
      "|ARGCY1Y1187B9A4FA5|            Gloriana|      Nashville, TN.|       36.16778|       -86.77836|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# discarding any duplicate artists by using distinct function\n",
    "\n",
    "artist_table_query = (spark.sql (\n",
    "    '''\n",
    "    select distinct\n",
    "    artist_id,\n",
    "    artist_name,\n",
    "    artist_location,\n",
    "    artist_latitude,\n",
    "    artist_longitude\n",
    "    from songs\n",
    "\n",
    "    '''\n",
    ")\n",
    ") \n",
    "\n",
    "artist_table_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdacd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a spark SQL table directly via SQL using CTAS\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "  CREATE TABLE songs_table\n",
    "  AS\n",
    "  select distinct\n",
    "    song_id,\n",
    "    title as song_title,\n",
    "    artist_id,\n",
    "    year,\n",
    "    duration\n",
    "    from songs\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa25ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  default|songs_table|      false|\n",
      "|         |      songs|       true|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirming if the table is craeated successfully. Below list will show both persistent table and temp view\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64230e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|          song_title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|ARDNS031187B9924F0|2005|186.48771|\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|ARDR4AC1187FB371A1|   0|511.16363|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (...|AREBBGV1187FB523D2|   0|173.66159|\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...|ARPBNLO1187FB3D52F|2000| 43.36281|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# querying the persistent table\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select*\n",
    "from songs_table\n",
    "\n",
    "'''\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55141c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|total_song_records|\n",
      "+------------------+\n",
      "|                71|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying number of songs created\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select\n",
    "count(*) as total_song_records\n",
    "from songs_table\n",
    "\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the table and writing to s3\n",
    "\n",
    "(\n",
    "   spark.table(\"songs_table\") # picking up the source table to write\n",
    "     .write\n",
    "     .mode(\"overwrite\")   # this mode replaces existing data. 'Apend' mode is used to add more data to existing data\n",
    "     .format(\"parquet\")   # Use \"csv\", \"json\", etc. for other formats. Parquet is recommended for datalakes\n",
    "     .option(\"path\", s3_output_songs_table) # specifiying s3 destination and outcome table name\n",
    "     .save() # executing the write operation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e28e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading all files in s3 log_data directory\n",
    "\n",
    "all_song_logs = spark.read.option(\"recursiveFileLookup\", \"true\").json(s3_logs_files)\n",
    "all_song_logs.printSchema()\n",
    "all_song_logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3e13bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a temporary sql view for data processing\n",
    "\n",
    "all_song_logs.createOrReplaceTempView(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3d1c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+----------+--------------------+---+----+-----+----+\n",
      "|                song|          artistName|       userName|userGender|           timestamp|day|hour|month|year|\n",
      "+--------------------+--------------------+---------------+----------+--------------------+---+----+-----+----+\n",
      "|       Sehr kosmisch|            Harmonia|     Ryan Smith|      Male|2018-11-15 00:30:...| 15|   0|   11|2018|\n",
      "|     The Big Gundown|         The Prodigy|     Ryan Smith|      Male|2018-11-15 00:41:...| 15|   0|   11|2018|\n",
      "|            Marry Me|               Train|     Ryan Smith|      Male|2018-11-15 00:45:...| 15|   0|   11|2018|\n",
      "|                NULL|                NULL|    Wyatt Scott|      Male|2018-11-15 01:57:...| 15|   1|   11|2018|\n",
      "|                NULL|                NULL| Austin Rosales|      Male|2018-11-15 03:29:...| 15|   3|   11|2018|\n",
      "|           Blackbird|         Sony Wonder|Samuel Gonzalez|      Male|2018-11-15 03:44:...| 15|   3|   11|2018|\n",
      "|                NULL|                NULL|Samuel Gonzalez|      Male|2018-11-15 03:44:...| 15|   3|   11|2018|\n",
      "|                NULL|                NULL|           NULL|      Male|2018-11-15 05:34:...| 15|   5|   11|2018|\n",
      "|                NULL|                NULL|   Tegan Levine|    Female|2018-11-15 05:37:...| 15|   5|   11|2018|\n",
      "|Best Of Both Worl...|           Van Halen|   Tegan Levine|    Female|2018-11-15 05:48:...| 15|   5|   11|2018|\n",
      "|Call Me If You Ne...|           Magic Sam|   Tegan Levine|    Female|2018-11-15 05:53:...| 15|   5|   11|2018|\n",
      "|                Home|Edward Sharpe & T...|   Tegan Levine|    Female|2018-11-15 05:55:...| 15|   5|   11|2018|\n",
      "|                 OMG|Usher featuring w...|   Tegan Levine|    Female|2018-11-15 06:01:...| 15|   6|   11|2018|\n",
      "|                NULL|                NULL|   Tegan Levine|    Female|2018-11-15 06:01:...| 15|   6|   11|2018|\n",
      "| Candle On The Water|         Helen Reddy|   Tegan Levine|    Female|2018-11-15 06:07:...| 15|   6|   11|2018|\n",
      "+--------------------+--------------------+---------------+----------+--------------------+---+----+-----+----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_query = (spark.sql (\n",
    "    '''\n",
    "    select \n",
    "    song,\n",
    "    artist as artistName,\n",
    "    concat(firstName,' ',lastName) as userName,\n",
    "    case when gender = 'F' then 'Female' else 'Male' end as userGender,\n",
    "    cast(ts/1000 as Timestamp) as timestamp,\n",
    "    day(cast(ts/1000 as Timestamp)) as day,\n",
    "    hour(cast(ts/1000 as Timestamp)) as hour,\n",
    "    month(cast(ts/1000 as Timestamp)) as month,\n",
    "    year(cast(ts/1000 as Timestamp)) as year\n",
    "    from logs\n",
    "\n",
    "    '''\n",
    ")\n",
    ") \n",
    "logs_query.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235690c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS log_table PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48db76f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a persistent table using CTAS\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "  CREATE TABLE logs_table\n",
    "  AS\n",
    "  select \n",
    "    song,\n",
    "    artist as artistName,\n",
    "    concat(firstName,' ',lastName) as userName,\n",
    "    case when gender = 'F' then 'Female' else 'Male' end as userGender,\n",
    "    cast(ts/1000 as Timestamp) as timestamp,\n",
    "    day(cast(ts/1000 as Timestamp)) as day,\n",
    "    hour(cast(ts/1000 as Timestamp)) as hour,\n",
    "    month(cast(ts/1000 as Timestamp)) as month,\n",
    "    year(cast(ts/1000 as Timestamp)) as year\n",
    "    from logs\n",
    "'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2069c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n",
      "|namespace|  tableName|isTemporary|\n",
      "+---------+-----------+-----------+\n",
      "|  default| logs_table|      false|\n",
      "|  default|songs_table|      false|\n",
      "|         |       logs|       true|\n",
      "|         |      songs|       true|\n",
      "+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirming if the table is craeated successfully. It will show you both persistent tables and temp views\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e35e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+--------------+----------+--------------------+---+----+-----+----+\n",
      "|           song| artistName|      userName|userGender|           timestamp|day|hour|month|year|\n",
      "+---------------+-----------+--------------+----------+--------------------+---+----+-----+----+\n",
      "|  Sehr kosmisch|   Harmonia|    Ryan Smith|      Male|2018-11-15 00:30:...| 15|   0|   11|2018|\n",
      "|The Big Gundown|The Prodigy|    Ryan Smith|      Male|2018-11-15 00:41:...| 15|   0|   11|2018|\n",
      "|       Marry Me|      Train|    Ryan Smith|      Male|2018-11-15 00:45:...| 15|   0|   11|2018|\n",
      "|           NULL|       NULL|   Wyatt Scott|      Male|2018-11-15 01:57:...| 15|   1|   11|2018|\n",
      "|           NULL|       NULL|Austin Rosales|      Male|2018-11-15 03:29:...| 15|   3|   11|2018|\n",
      "+---------------+-----------+--------------+----------+--------------------+---+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# querying the persistent table\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select*\n",
    "from logs_table\n",
    "\n",
    "'''\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5cd1c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|total_logs_created|\n",
      "+------------------+\n",
      "|              8056|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying number of logs created\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select\n",
    "count(*) as total_logs_created\n",
    "from logs_table\n",
    "\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ae7a0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the table and writing to s3\n",
    "\n",
    "(\n",
    "   spark.table(\"logs_table\") # picking up the source table to write\n",
    "     .write\n",
    "     .mode(\"overwrite\")   # this mode replaces existing data. 'Apend' mode is used to add more data to existing data\n",
    "     .format(\"parquet\")   # Use \"csv\", \"json\", etc. for other formats. Parquet is recommended for datalakes\n",
    "     .option(\"path\", s3_output_logs_table) # specifiying s3 destination and outcome table name\n",
    "     .save() # executing the write operation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "56e821a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joining both tables by using inner join (unique key = song_title)\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "  CREATE TABLE combined_processed_dataset\n",
    "  AS\n",
    "  select\n",
    "  b.song_title,\n",
    "  b.song_id,\n",
    "  b.duration,\n",
    "  a.artistName,\n",
    "  a.userName,\n",
    "  a.timestamp,\n",
    "  a.hour,\n",
    "  a.day,\n",
    "  a.month,\n",
    "  a.year\n",
    "  \n",
    "  from\n",
    "  logs_table a\n",
    "  inner join songs_table b\n",
    "  on a.song =b.song_title \n",
    "    \n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0b100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS your-table-name PURGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "69fcfcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|combined_processe...|      false|\n",
      "|  default|          logs_table|      false|\n",
      "|  default|         songs_table|      false|\n",
      "|         |                logs|       true|\n",
      "|         |               songs|       true|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirming if the table is craeated successfully. It will show you both persistent tables and temp views\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "af378bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+---------+-----------------+-------------+--------------------+----+---+-----+----+\n",
      "|    song_title|           song_id| duration|       artistName|     userName|           timestamp|hour|day|month|year|\n",
      "+--------------+------------------+---------+-----------------+-------------+--------------------+----+---+-----+----+\n",
      "|Setanta matins|SOZCTXZ12AB0182364|269.58322|            Elena|    Lily Koch|2018-11-21 21:56:...|  21| 21|   11|2018|\n",
      "|         Intro|SOGDBUF12A8C140FAA| 75.67628|        Percubaba|  Sylvie Cruz|2018-11-14 05:06:...|   5| 14|   11|2018|\n",
      "|         Intro|SOGDBUF12A8C140FAA| 75.67628|Calvin Richardson|Layla Griffin|2018-11-19 09:14:...|   9| 19|   11|2018|\n",
      "|         Intro|SOGDBUF12A8C140FAA| 75.67628|      Samy Deluxe| Tegan Levine|2018-11-27 22:35:...|  22| 27|   11|2018|\n",
      "+--------------+------------------+---------+-----------------+-------------+--------------------+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# querying the persistent table\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select*\n",
    "from combined_processed_dataset\n",
    "\n",
    "'''\n",
    ").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8f1e8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|            4|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verifying number of records\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "select\n",
    "count(*) as total_records\n",
    "from combined_processed_dataset\n",
    "\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "271c1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the table and writing to s3\n",
    "\n",
    "(\n",
    "   spark.table(\"combined_processed_dataset\") # picking up the source table to write\n",
    "     .write\n",
    "     .mode(\"overwrite\")   # this mode replaces existing data. 'Apend' mode is used to add more data to existing data\n",
    "     .format(\"parquet\")   # Use \"csv\", \"json\", etc. for other formats. Parquet is recommended for datalakes\n",
    "     .option(\"path\", s3_output_combined_table) # specifiying s3 destination and outcome table name\n",
    "     .save() # executing the write operation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "29bada30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark session is stopped sucessfully\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print ('spark session is stopped sucessfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
